{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21ed714",
   "metadata": {},
   "source": [
    "# 행동 스티커 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb36656",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca3fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip install loguru\n",
    "# $ pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9ca26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac26/anaconda3/envs/aiffel/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "2021-05-22 01:53:01,489\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' #CPU 사용\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "workdir = os.path.join(os.getenv('HOME'),'aiffel/mpii')\n",
    "os.chdir(workdir)\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c25d70",
   "metadata": {},
   "source": [
    "### json 파싱하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4741f8",
   "metadata": {},
   "source": [
    "이전 스텝에서 train.json과 validation.json 파일을 다운로드받은 것을 기억하시나요? 이 파일들은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있어서 Pose Estimation을 위한 label로 삼을 수 있습니다.\n",
    "\n",
    "우선 json이 어떻게 구성되어 있는지 파악해 보기 위해 json 파일을 열어 샘플로 annotation 정보를 1개만 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc649f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "json_file_path = os.getenv('HOME')+'/aiffel/mpii/mpii_human_pose_v1_u12_2/train.json'\n",
    "\n",
    "with open(json_file_path) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2) # json beautify\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26104ca",
   "metadata": {},
   "source": [
    "* json annotation 을 파싱하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0958365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22795b",
   "metadata": {},
   "source": [
    "image 의 전체 path 를 묶어 dict 타입의 label 로 만들어 냅니다. 이 label 을 가지고 학습을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9753d",
   "metadata": {},
   "source": [
    "## tfrecord 파일 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a64c2",
   "metadata": {},
   "source": [
    "이전까지는 tf.keras 의 imagedatagenerator 를 이용해서 주로 학습데이터를 읽었습니다. 하지만 실제 프로젝트에서는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다뤄야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938f2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac0913",
   "metadata": {},
   "source": [
    "annotation 을 total_shards 개수로 나눔(chunkify) (train : 64개, val : 8개)  \n",
    "build_single_tfrecord 함수를 통해 tfrecord 로 저장  \n",
    "각 chunk 끼리 dependency 가 없기 때문에 병렬처리가 가능, ray를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ea08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b397a4",
   "metadata": {},
   "source": [
    "l 은 annotation, n은 shard 개수  \n",
    "shard 개수 단위로 annotation list 를 나누어서 새로운 list를 만듭니다.  \n",
    "numpy array 라고 가정하면 (size, shard, anno_content) 정도의 shape을 가지겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8188f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = genreate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd150b23",
   "metadata": {},
   "source": [
    "TFRecordWriter 를 이용해서 anno_list 를 shard 개수 단위로 작성합니다.  \n",
    "generate_tfexample 함수를 사용합니다. → 아래에서 자세히 설명하겠습니다.  \n",
    "[중요] write 할 때 string 으로 serialize 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db9bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5b5d8",
   "metadata": {},
   "source": [
    "우리가 정의한 json 의 python type의 값들을 tfexample 에 사용할 수 있는 값으로 변환합니다.  \n",
    "image 파일은 byte 로 변환합니다. bitmap 으로 저장하게되면 파일용량이 상당히 커지기 때문에 만약 jpeg 타입이 아닌 경우 jpeg 으로 변환 후 content 로 불러서 저장합니다. (H,W,C)  \n",
    "각 label 값을 tf.train.Feature 로 저장합니다. 이 때 데이터 타입에 주의해야 합니다.  \n",
    "이미지는 byte 인코딩 된 값을 그대로 넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c935fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d456b5c",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09eff44",
   "metadata": {},
   "source": [
    "파이썬을 위한 간단한 분산 어플리케이션 api  \n",
    "https://docs.ray.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a00c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecords_mpii.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e0b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-22 01:54:14,152\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10431)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10428)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10429)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10422)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10420)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10430)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10424)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10427)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10426)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10425)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10423)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=10421)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    # x = [\n",
    "    #     joint[0] / width if joint[0] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    # y = [\n",
    "    #     joint[1] / height if joint[1] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        # 'image/object/parts/x':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=x)),\n",
    "        # 'image/object/parts/y':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Start to parse annotations.')\n",
    "    if not os.path.exists('./tfrecords_mpii'):\n",
    "        os.makedirs('./tfrecords_mpii')\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/train.json') as train_json:\n",
    "        train_annos = json.load(train_json)\n",
    "        train_annotations = [\n",
    "            parse_one_annotation(anno, './images/')\n",
    "            for anno in train_annos\n",
    "        ]\n",
    "        print('First train annotation: ', train_annotations[0])\n",
    "        del (train_annos)\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/validation.json') as val_json:\n",
    "        val_annos = json.load(val_json)\n",
    "        val_annotations = [\n",
    "            parse_one_annotation(anno, './images/') for anno in val_annos\n",
    "        ]\n",
    "        print('First val annotation: ', val_annotations[0])\n",
    "        del (val_annos)\n",
    "\n",
    "    print('Start to build TF Records.')\n",
    "    build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "    build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "    print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "        len(train_annotations) + len(val_annotations)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b16260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecord 생성을 수행\n",
    "# $ cd ~/aiffel/mpii && python tfrecords_mpii.py\n",
    "# 결과\n",
    "# $ cd ~/aiffel/mpii/tfrecords_mpii && ls | wc\n",
    "# 약 200MB 정도의 tfrecords들이 72개 만들어진 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50ab8f",
   "metadata": {},
   "source": [
    "## data label 로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e4ae3",
   "metadata": {},
   "source": [
    "tfrecords 파일을 읽고 전처리를 할 수 있는 dataloader 를 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776bd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y)\n",
    "\n",
    "        # print (image.shape, heatmaps.shape, type(heatmaps))\n",
    "\n",
    "        return image, heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9cc29",
   "metadata": {},
   "source": [
    "위 Preprocessor 클래스 코드에서 __call__() 메소드 내부에서 진행되는 주요 과정을 정리하면 아래와 같습니다.\n",
    "\n",
    "\n",
    "tfrecord 파일이기 때문에 병렬로 읽는 것은 tf 가 지원해주고 있습니다. self.parse_tfexample() 에 구현되어 있고 이 함수를 통해 tf.tensor 로 이루어진 dictionary 형태의 features를 얻을 수 있습니다.  \n",
    "즉 image 는 features['image/encoded'] 형태로 사용할 수 있고 tfrecord 를 저장할 때 jpeg encoding 된 값을 넣었으므로 tf.io.decode_jpeg()로 decoding 하여 tensor 형태의 이미지를 얻습니다.  \n",
    "crop_roi() 메소드를 이용해 해당 이미지를 학습하기 편하도록 몇가지 트릭을 적용합니다. 구현은 아래에서 다시 소개하겠습니다.  \n",
    "make_heatmaps() 메소드를 이용해 label을 heatmap 으로 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f40a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(self, example_proto):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'imaage/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto,\n",
    "                                      image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36791926",
   "metadata": {},
   "source": [
    "tfrecord 파일 형식을 우리가 저장한 data type feature 에 맞게 parsing 합니다.  \n",
    "tf 가 자동으로 parsing 해주는 점은 아주 편하지만 feature description 을 정확하게 알고 있어야하는 단점이 있습니다. 즉, tfrecord 에서 사용할 key 값들과 data type 을 모르면 tfrecord 파일을 사용하기 굉장히 어렵습니다. (serialize 되어있으므로..)\n",
    "\n",
    "이렇게 얻은 image 와 label 을 이용해서 적절한 학습형태로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19547ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(self, image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    # shift all keypoints based on the crop area\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58e10a",
   "metadata": {},
   "source": [
    "우리가 알고 있는 것은 joints 의 위치, center 의 좌표, body height 값  \n",
    "균일하게 학습하기 위해 body width 를 적절히 정하는 것도 중요  \n",
    "높이 정보와 keypoint 위치를 이용해서 정사각형 박스를 사용하는 것을 기본으로 디자인  \n",
    "임의로 조정한 crop box 가 이미지 바깥으로 나가지 않는지 예외 처리를 잘 해주어야 한다  \n",
    "(x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c274f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmaps(self, features, keypoint_x, keypoint_y):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * self.heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * self.heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = self.heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(self.heatmap_shape[1], self.heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef7ddf",
   "metadata": {},
   "source": [
    "16개의 점을 generate_2d_gaussian() 함수를 이용해서 64x64 의 map 에 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff244f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    \"\"\"\n",
    "    \"The same technique as Tompson et al. is used for supervision. A MeanSquared Error (MSE) loss is\n",
    "    applied comparing the predicted heatmap to a ground-truth heatmap consisting of a 2D gaussian\n",
    "    (with standard deviation of 1 px) centered on the keypoint location.\"\n",
    "\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/util/img.lua#L204\n",
    "    \"\"\"\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    # this gaussian patch is 7x7, let's get four corners of it first\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    # if the patch is out of image boundary we simply return nothing according to the source code\n",
    "    # [1]\"In these cases the joint is either truncated or severely occluded, so for\n",
    "    # supervision a ground truth heatmap of all zeros is provided.\"\n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "    # the center of the gaussian patch should be 1\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    # generate this 7x7 gaussian patch\n",
    "    gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "    # part of the patch could be out of the boundary, so we need to determine the valid range\n",
    "    # if xmin = -2, it means the 2 left-most columns are invalid, which is max(0, -(-2)) = 2\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    # if xmin = 59, xmax = 66, but our output is 64x64, then we should discard 2 right-most columns\n",
    "    # which is min(64, 66) - 59 = 5, and column 6 and 7 are discarded\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    # also, we need to determine where to put this patch in the whole heatmap\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    # finally, insert this patch into the heatmap\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6fccd",
   "metadata": {},
   "source": [
    "igma 값이 1 이고 window size 7 인 필터를 이용해서 만들었습니다. 이런 특수 함수들은 공개되어 있는 구현이 많기 때문에 참고해서 사용하는 것을 추천  \n",
    "모두 연결해서 py로 만들면 preprocess.py 모듈 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd9b91",
   "metadata": {},
   "source": [
    "## 모델을 학습해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b657e8c",
   "metadata": {},
   "source": [
    "* Hourglass 모델 만들기 (hourglass104.py 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d0fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50d006c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ssac26/aiffel/mpii'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05732af4",
   "metadata": {},
   "source": [
    "![model](./image/deeper20_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0d2e8",
   "metadata": {},
   "source": [
    "직육면체 박스는 residual block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd106477",
   "metadata": {},
   "source": [
    "* Residual block module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f4290",
   "metadata": {},
   "source": [
    " 기존 resnet과 다르게 batchnorm, relu를 먼저사용한다??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96085b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1x1-3x3-1x1 bottleneck block\n",
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,  # lift channels first\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37087571",
   "metadata": {},
   "source": [
    "* Hourglass module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4804cf",
   "metadata": {},
   "source": [
    "재귀함수 이용해서 모델구성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4f6f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L3\n",
    "    \"\"\"\n",
    "    # Upper branch\n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    # Lower branch\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580248e",
   "metadata": {},
   "source": [
    "바깥부터 5개의 양파껍질(층)을 만들고 싶다면 order 를 이용해서 5,4…1 이 될때까지 HourglassModule 을 반복하면 order 가 1이 되면 BottleneckBlock 으로 대체해주면 아주 간결하게 만들 수 있습니다.\n",
    "\n",
    "이 hourglass 모듈을 여러 층으로 쌓은 것이 stacked hourglass network 인데요, 모델이 깊어지는 만큼 학습이 어려워 intermediate loss (auxilary loss) 를 추가해야하는 것을 논문에서 언급 했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc4c0f",
   "metadata": {},
   "source": [
    "* intermediate output을 위한 linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a66b84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30182785",
   "metadata": {},
   "source": [
    "따라서 stacked 되는 hourglass 층 사이사이에 LinearLayer 를 삽입하고 중간 loss 를 계산해줍니다.\n",
    "\n",
    "지금까지 만든 hourglass 를 여러층으로 쌓으면 stacked hourglass 가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682b983",
   "metadata": {},
   "source": [
    "* Stacked Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca067449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), num_stack=4, num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L33\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # initial processing of the image\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        # predict 256 channels like a fully connected layer.\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        # predict final channels, which is also the number of predicted heatmap\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        # if it's not the last stack, we need to add predictions back\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26af681",
   "metadata": {},
   "source": [
    "지금까지의 함수가 적용된 hourglass104.py파일 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8cd94",
   "metadata": {},
   "source": [
    "## 학습 엔진 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabf3d5",
   "metadata": {},
   "source": [
    "학습 코드 train.py를 구현, 지금까지 만든 py파일들 import해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dcf7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a1c11",
   "metadata": {},
   "source": [
    " gpu memory growth 옵션을 조정하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "262a2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ce26d",
   "metadata": {},
   "source": [
    "### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afacb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed949d",
   "metadata": {},
   "source": [
    "loss : MSE (heatmap 을 pixel 단위 MSE 로 계산) → 실제 계산은 약간 달라요! compute_loss() 에서 새로 구현합니다.  \n",
    "strategy : 분산학습용 tf.strategy 입니다. 사용 가능한 GPU가 1개뿐이라면 사용하지 않아요.  \n",
    "optimizer : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78170919",
   "metadata": {},
   "source": [
    "### learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7909d58",
   "metadata": {},
   "source": [
    " decay step 에 따라 1/10 씩 작아지도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5a03699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(self):\n",
    "    \"\"\"\n",
    "    This effectively simulate ReduceOnPlateau learning rate schedule. Learning rate\n",
    "    will be reduced by a factor of 5 if there's no improvement over [max_patience] epochs\n",
    "    \"\"\"\n",
    "\n",
    "    if self.patience_count >= self.max_patience:\n",
    "        self.current_learning_rate /= 10.0\n",
    "        self.patience_count = 0\n",
    "    elif self.last_val_loss == self.lowest_val_loss:\n",
    "        self.patience_count = 0\n",
    "    self.patience_count += 1\n",
    "\n",
    "    self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "def lr_decay_step(self, epoch):\n",
    "    if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "        self.current_learning_rate /= 10.0\n",
    "    self.optimizer.learning_rate = self.current_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba2d03",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74a79a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, labels, outputs):\n",
    "    loss = 0\n",
    "    for output in outputs:\n",
    "        # assign more weights to foreground pixels\n",
    "        weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "        loss += tf.math.reduce_mean(\n",
    "            tf.math.square(labels - output) * weights) * (\n",
    "                1. / self.global_batch_size)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2addf",
   "metadata": {},
   "source": [
    "self.loss_object 를 사용해서 MSE 로 구현하는 것이 맞지만 사실 동일 weight MSE 는 수렴이 잘 되지 않습니다. 예측해야하는 positive (joint 들) 의 비율이 negative (배경이라고 할 수 있겠죠?) 에 비해 상당히 적은 비율로 등장하기 때문인데요. 이 때문에 실제 구현에서는 약간의 테크닉을 추가해줄 필요가 있습니다. label 이 배경이 아닌 경우 (heatmap 값이 0보다 큰 경우) 에 추가적인 weight 를 주면 보다 나아지는 경향을 볼 수 있었습니다. weight 가 82인 이유는 heatmap 전체 크기인 64x64 에서 gaussian point 등장 비율이 7x7 패치이기 때문에 64 / 7 = 9.1 ⇒ 9x9 로 계산해 봤습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc38d12",
   "metadata": {},
   "source": [
    "tf.gradienttape 을 이용해 loss 를 업데이트 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4857d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, inputs):\n",
    "    images, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = self.model(images, training=True)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "    grads = tape.gradient(\n",
    "        target=loss, sources=self.model.trainable_variables)\n",
    "    self.optimizer.apply_gradients(\n",
    "        zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def val_step(self, inputs):\n",
    "    images, labels = inputs\n",
    "    outputs = self.model(images, training=False)\n",
    "    loss = self.compute_loss(labels, outputs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746822",
   "metadata": {},
   "source": [
    "실제 학습하는 함수입니다. distributed_train_epoch() 과 distributed_val_epoch() 함수는 gpu를 여러개 이용하는 분산 학습용 코드이니, 사용하지 않더라도 참고삼아 봐두시길 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e423e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (<ipython-input-39-78f622f78b7f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-78f622f78b7f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def distributed_train_epoch(dataset):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "def run(self, train_dist_dataset, val_dist_dataset):\n",
    "    @tf.function\n",
    "def distributed_train_epoch(dataset):\n",
    "    tf.print('Start distributed traininng...')\n",
    "    total_loss = 0.0\n",
    "    num_train_batches = 0.0\n",
    "    for one_batch in dataset:\n",
    "        per_replica_loss = self.strategy.experimental_run_v2(\n",
    "            self.train_step, args=(one_batch, ))\n",
    "        batch_loss = self.strategy.reduce(\n",
    "            tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "        total_loss += batch_loss\n",
    "        num_train_batches += 1\n",
    "        tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                 batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "    return total_loss, num_train_batches\n",
    "\n",
    "    @tf.function\n",
    "def distributed_val_epoch(dataset):\n",
    "    total_loss = 0.0\n",
    "    num_val_batches = 0.0\n",
    "    for one_batch in dataset:\n",
    "        per_replica_loss = self.strategy.experimental_run_v2(\n",
    "            self.val_step, args=(one_batch, ))\n",
    "        num_val_batches += 1\n",
    "        batch_loss = self.strategy.reduce(\n",
    "            tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "        tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                 batch_loss)\n",
    "        if not tf.math.is_nan(batch_loss):\n",
    "            # TODO: Find out why the last validation batch loss become NaN\n",
    "            total_loss += batch_loss\n",
    "        else:\n",
    "            num_val_batches -= 1\n",
    "\n",
    "    return total_loss, num_val_batches\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "summary_writer.set_as_default()\n",
    "\n",
    "for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "    tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "    self.lr_decay()\n",
    "    tf.summary.scalar('epoch learning rate',\n",
    "                      self.current_learning_rate)\n",
    "\n",
    "    print('Start epoch {} with learning rate {}'.format(\n",
    "        epoch, self.current_learning_rate))\n",
    "\n",
    "    train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "        train_dist_dataset)\n",
    "    train_loss = train_total_loss / num_train_batches\n",
    "    print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "    tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "    val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "        val_dist_dataset)\n",
    "    val_loss = val_total_loss / num_val_batches\n",
    "    print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "    tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "    # save model when reach a new lowest validation loss\n",
    "    if val_loss < self.lowest_val_loss:\n",
    "        self.save_model(epoch, val_loss)\n",
    "        self.lowest_val_loss = val_loss\n",
    "    self.last_val_loss = val_loss\n",
    "\n",
    "return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, epoch, loss):\n",
    "    model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "        self.version, epoch, loss)\n",
    "    self.model.save_weights(model_name)\n",
    "    self.best_model = model_name\n",
    "    print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c48992",
   "metadata": {},
   "source": [
    "### tf.dataset 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463c11b",
   "metadata": {},
   "source": [
    " tfrecord 파일을 tf.dataset 으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58350729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79020588",
   "metadata": {},
   "source": [
    "preprocessor 구현에서 tfrecord 규칙을 모두 정의했기 때문에 단순히 tfrecord list 을 읽어와서 tf.data API 에 입력한 후, preprocessor 를 map 으로 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571f042",
   "metadata": {},
   "source": [
    "### train함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d5a4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0521c8",
   "metadata": {},
   "source": [
    "train.py의 메인 실행부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     tfrecords_dir = './dataset/tfrecords_mpii/'\n",
    "#     train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "#     val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "#     epochs = 50\n",
    "#     batch_size = 16\n",
    "#     num_heatmap = 16\n",
    "#     tensorboard_dir = './logs/'\n",
    "#     learning_rate = 0.0007\n",
    "#     start_epoch = 1\n",
    "\n",
    "#     automatic_gpu_usage()\n",
    "\n",
    "#     pretrained_path = None\n",
    "\n",
    "#     train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "#           num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe6243",
   "metadata": {},
   "source": [
    "완성된  train.py 실행 >> weight 파일까지 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee757b",
   "metadata": {},
   "source": [
    "## 둠칫둠칫 댄스타임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a629c",
   "metadata": {},
   "source": [
    "예측 엔진 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506cfd8a",
   "metadata": {},
   "source": [
    "test.py 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d531d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "automatic_gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9231a",
   "metadata": {},
   "source": [
    "학습한 weight 로 예측을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), num_stack=4, num_residual=1,\n",
    "        num_heatmap=16)\n",
    "\n",
    "model.load_weights('./models/model-v0.0.3-epoch-1-loss-1.0744.h5')  # 본인이 학습한 weight path로 바꿔주세요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b01d22",
   "metadata": {},
   "source": [
    "### heatmap to coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d308a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (4096, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df871144",
   "metadata": {},
   "source": [
    "64x64 heatmap 으로 나오기 때문에 최대값을 찾는 함수가 필요합니다. 64x64 를 fatten 후 argmax index 를 찾습니다. 64x64 이미지 이기 때문에 row 와 col 값을 몫과 나머지로 표현하면 쉽게 값을 얻을 수 있습니다.\n",
    "\n",
    "위 방법만으로는 256x256 이미지에 64x64 heatmap max 값을 표현하려면 quantization 오차가 발생하기 때문에 실제 계산에서는 3x3 필터를 이용해서 근사치를 구해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3606362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        # since we've padded the heatmap, the max keypoint should increment by 1\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        # the patch is the 3x3 grid around the max keypoint location\n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        # assign 0 to max location\n",
    "        patch[1][1] = 0\n",
    "        # and the next largest value is the largest neigbour we are looking for\n",
    "        index = np.argmax(patch)\n",
    "        # find out the location of it relative to center\n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        # we can then add original max keypoint location with this offset\n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "    # we do need to clip the value to make sure there's no keypoint out of border, just in case.\n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    # normalize the points so that we can scale back easily\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8784b",
   "metadata": {},
   "source": [
    "### 예측함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f931356",
   "metadata": {},
   "source": [
    "### keypoint 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00207ea",
   "metadata": {},
   "source": [
    "### skeleton그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f77dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    # draw skeleton\n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48fd1b",
   "metadata": {},
   "source": [
    "### 결과 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, keypoints = predict('./test_image.jpg')\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d896ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 수행 결과를 터미널에서 확인하려면 아래와 같이 실행해 주세요. test.py에서 predict() 함수에 다양한 이미지를 대입해 보면서 pose estimation의 결과를 비교해 보시기 바랍니다.\n",
    "\n",
    "# $ cd ~/aiffel/mpii && python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254d259",
   "metadata": {},
   "source": [
    "Project: 모델 바꿔보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267913f",
   "metadata": {},
   "source": [
    "simplebaseline 모델로 변경해 봅시다\n",
    "지금까지 우리는 StackedHourglass Network 기반으로 학습을 진행해 왔습니다.\n",
    "\n",
    "그러나 지난시간에 살펴본 것처럼 Simplebaseline 모델은 이보다 훨씬 간단한 모델 구조에도 불구하고 더욱 좋은 성능을 보여주었던 바 있습니다.\n",
    "\n",
    "실제로도 그런 성능을 얻을 수 있을지 확인해 보겠습니다.\n",
    "\n",
    "STEP 1 : simplebaseline 모델 완성하기\n",
    "simplebaseline.py 파일 내용을 완성합니다.\n",
    "\n",
    "STEP 2 : simplebaseline 모델로 변경하여 훈련하기\n",
    "train.py 218라인의 모델 선언 부분을 simplebaseline 모델로 변경한 후 다시 학습을 진행합니다.\n",
    "\n",
    "STEP 3 : 두 모델의 비교\n",
    "실습에서 다룬 StackedHourglass Network와 Simplebaseline 모델을 둘 다 동일한 Epoch 수만큼 학습하여 그 결과를 비교해 봅니다.\n",
    "\n",
    "Pose Estimation 결과 시각화 (정성적 비교)\n",
    "학습 진행경과 (loss 감소현황)\n",
    "가급적 두 모델 공히 최소 3epoch이상, (5epoch 이상 권장)을 학습하기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CV deeper노드를 모두 종료했지만 점점 어려워지고 이전과 달리 노드내용을 따라가는데도 실습이 제대로 안되는 현상도 발생해서 아쉽다. 이해부족으로 프로젝트 진행불가'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d0912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bf457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140ee38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00062dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
